2. Theoretische Grundlagen
2.1 Zieldefinierung KPIs
Key Performance Indicators (KPIs) sind zentrale Steuerungsinstrumente im Performance Management, mit denen der Fortschritt strategischer und operativer Ziele quantitativ erfasst wird. Sie fungieren als objektive Maßstäbe, um Abweichungen frühzeitig zu identifizieren und notwendige Korrekturmaßnahmen einzuleiten. Eine präzise Zieldefinierung legt dabei den Grundstein für die Relevanz und Aussagekraft der Kennzahlen, denn KPIs ohne klar umrissene Zielgrößen liefern weder verlässliche Steuerungsimpulse noch eine valide Entscheidungsgrundlage für das Management (Kaplan & Norton, 1996; Neely, 2005).
Zur Sicherstellung einer effizienten Zielverfolgung hat sich die SMART-Formel etabliert, wonach Ziele spezifisch, messbar, attraktiv, realistisch und terminiert sein müssen. Die Spezifizierung garantiert, dass KPI-Werte eindeutig interpretierbar sind, während die Messbarkeit eine kontinuierliche Nachverfolgung ermöglicht. Attraktivität und Realisierbarkeit stellen sicher, dass die definierten Ziele motivierend wirken und gleichzeitig innerhalb der verfügbaren Ressourcen erreichbar bleiben. Die Terminvorgabe schafft zeitliche Orientierung und vermeidet eine schleichende Entwertung der Indikatoren im Zeitverlauf (Doran, 1981; Bourne, Kennerley & Franco, 2002).
Die Auswahl geeigneter KPIs muss im Kontext der Unternehmensstrategie und der jeweiligen Funktionsbereiche erfolgen. So unterscheiden sich finanzielle, prozessbezogene und kundenorientierte KPIs nicht nur inhaltlich, sondern auch in ihrer Auswirkung auf strategische Entscheidungen. Ein Balance-Ansatz, wie er etwa im Performance Prism vorgeschlagen wird, empfiehlt die gleichgewichtete Berücksichtigung unterschiedlicher Stakeholder-Perspektiven und stellt damit sicher, dass eine ganzheitliche Steuerung erreicht wird. Dabei werden strategische Ziele in operative Teilziele heruntergebrochen und KPI-Hierarchien gebildet, die eine durchgängige Nachverfolgbarkeit über alle Managementebenen hinweg ermöglichen (Neely, Adams & Kennerley, 2002).
Schließlich unterliegt die Zieldefinierung einem kontinuierlichen Verbesserungsprozess. KPIs sollten regelmäßig auf ihre Relevanz und ihre Zielerreichung hin überprüft und gegebenenfalls angepasst werden. Dieser iterative Ansatz ist im Qualitätsmanagement fest verankert und findet etwa in der Norm ISO 9001:2015 seinen Niederschlag, die Leistungsindikatoren als integralen Bestandteil des Plan-Do-Check-Act-Zyklus definiert. Durch die Etablierung von Governance-Strukturen und Reporting-Routinen wird sichergestellt, dass KPI-Ziele nicht nur gesetzt, sondern auch systematisch validiert und weiterentwickelt werden (ISO 9001:2015; Bourne et al., 2002).
Key Performance Indicators (KPIs) sind zentrale Steuerungsinstrumente des Performance Managements. Sie dienen der Messung strategischer wie operativer Zielerreichung und stellen sicher, dass Abweichungen frühzeitig erkannt und korrigiert werden können.¹ Eine präzise Zieldefinition bildet den Grundstein für die Aussagekraft der Kennzahlen, da unscharfe oder unklare Zielgrößen keine verlässlichen Steuerungsimpulse liefern.²

Als etabliertes Prinzip gilt die SMART-Formel: Ziele sollten spezifisch, messbar, attraktiv, realistisch und terminiert sein.³ Diese Kriterien garantieren, dass KPI-Werte eindeutig interpretierbar sind, motivierend wirken und innerhalb vorhandener Ressourcen erreichbar bleiben. Die Terminvorgabe verhindert zudem eine schleichende Entwertung der Indikatoren.

In der Praxis erfolgt die Auswahl geeigneter KPIs im Einklang mit der Unternehmensstrategie. Während finanzielle, prozessuale und kundenorientierte Kennzahlen unterschiedliche Schwerpunkte setzen, empfiehlt die Balanced Scorecard einen balancierten Ansatz über mehrere Dimensionen.⁴ Ergänzend zeigt das Performance Prism, dass Stakeholder-Perspektiven stärker integriert werden müssen, um eine ganzheitliche Steuerung sicherzustellen.⁵

KPIs sind nicht statisch, sondern Teil eines kontinuierlichen Verbesserungsprozesses. Sie werden regelmäßig überprüft und an neue Markt- oder Unternehmensbedingungen angepasst. In diesem Zusammenhang spielt auch das Qualitätsmanagement (z. B. ISO 9001) eine Rolle, das Kennzahlen als integralen Bestandteil des PDCA-Zyklus versteht.⁶
2.2 Abweichungsanalyse
Die Ist-Soll-Analyse bildet eine methodische Grundlage zur systematischen Identifikation von Optimierungspotenzialen in Geschäftsprozessen und IT-Landschaften. Sie beinhaltet die detaillierte Erfassung des aktuellen Zustands (Ist) sowie die Definition eines idealen Zielzustands (Soll) und deren Gegenüberstellung zur Aufdeckung von Abweichungen. Durch diese strukturierte Vorgehensweise lassen sich nicht nur Effizienzsteigerungen und Kosteneinsparungen realisieren, sondern auch strategische Anpassungen an veränderte Markt- und Compliance-Anforderungen planen.
Die Ist-Analyse beginnt mit der Erhebung quantitativer und qualitativer Daten zu bestehenden Abläufen und Systemen. Wesentliche Quellen sind Organisationsdokumente, IT-Systemprotokolle sowie Experteninterviews mit Führungskräften und operativen Mitarbeitenden. Anschließend werden Prozesse mithilfe von Techniken wie Prozesslandkarten, Zeit-Mengen-Analysen oder Reifegradmodellen (z. B. CMMI) bewertet, um Schwachstellen und Engpässe zu identifizieren. Diese umfassende Datengrundlage dient als objektives Abbild der Ist-Situation und bildet die Basis für die Ableitung konkreter Anforderungen an den Soll-Zustand.
In der Soll-Analyse werden auf Basis der Unternehmensstrategie, regulatorischer Vorgaben und Nutzeranforderungen Zielprozesse und Systemlandschaften modelliert. Häufig kommen hierfür standardisierte Modellierungssprachen wie BPMN oder UML zum Einsatz, um Zielabläufe formal zu beschreiben und visualisieren. Ein klar definiertes Soll-Modell unterstützt die Kommunikation aller Stakeholder und gewährleistet, dass Optimierungsmaßnahmen den definierten Anforderungen genügen und Rückkopplungsschleifen minimiert werden.
Der Soll-Ist-Vergleich, oft auch Gap-Analyse genannt, quantifiziert die Abweichungen zwischen Ist- und Soll-Zustand. Die identifizierten Lücken werden anhand von Kriterien wie Dringlichkeit, Komplexität und erwarteter Nutzen priorisiert. Kurzfristige Quick Wins werden von langfristigen Transformationsprojekten unterschieden, um eine realistische Roadmap für die Implementierung zu erstellen. Verantwortlichkeiten und Meilensteine werden dabei in einem Maßnahmenplan festgehalten, um eine transparente Steuerung und Kontrolle des Veränderungsprozesses sicherzustellen.
Im Kontext digitaler Transformationsvorhaben übernimmt die Ist-Soll-Analyse eine Schlüsselrolle, da sie betriebswirtschaftliche Zielsetzungen mit technischer Umsetzbarkeit verknüpft. Sie ermöglicht es, IT-Projekte zielgerichtet zu strukturieren, Risiken frühzeitig zu erkennen und die Akzeptanz bei Anwendern durch transparente Abstimmungsprozesse zu erhöhen. Darüber hinaus schafft sie eine belastbare Entscheidungsgrundlage für Investitionen in Software, Hardware und Schulungsmaßnahmen.
Zu den zentralen Herausforderungen der Ist-Soll-Analyse zählen unvollständige oder veraltete Daten, heterogene Systemlandschaften und kulturelle Widerstände in der Belegschaft. Ein interdisziplinäres Projektteam, iteratives Vorgehen und fortlaufende Stakeholder-Einbindung sind daher essenziell, um Informationslücken zu schließen und den Veränderungsprozess nachhaltig zu gestalten. Nur so kann die Analyse ihr Potenzial als Steuerungsinstrument für effiziente und zukunftsfähige Geschäftsprozesse vollständig entfalten.
Copilot
2.3 Ursachenforschung
5 Whys
Die 5-Why-Methode, auch als Fünf-Warum-Technik bekannt, geht auf Toyoda Sakichi und das Toyota Production System zurück und wurde entwickelt, um durch konsequentes Nachfragen nach dem „Warum?“ von oberflächlichen Symptomen zu den tieferliegenden Ursachen zu gelangen. Dabei stellt ein interdisziplinäres Team ein klar umrissenes Problem zunächst als Ausgangspunkt auf und leitet aus jeder Antwort auf die Frage „Warum?“ unmittelbar die nächste Frage ab. Der symbolische Wert von fünf Wiederholungen soll sicherstellen, dass der Analyseprozess so lange fortgeführt wird, bis eine nicht weiter aufteilbare Grundursache gefunden ist1.
Im praktischen Ablauf beginnt die Methode mit der präzisen Problemformulierung, etwa dem wiederholten Ausfall eines Softwaremoduls. Anschließend werden in strukturierter Folge fünf „Warum?“-Fragen gestellt, deren Antworten jeweils in chronologischer Reihenfolge dokumentiert werden. Durch diese lineare Kausalkette lassen sich nicht nur technische Fehler, sondern auch organisatorische Schwachstellen identifizieren. Zu den Stärken zählen die geringe Komplexität, die Förderung der bereichsübergreifenden Zusammenarbeit und die schnelle Umsetzbarkeit ohne aufwändige statistische Auswertungen. Gleichwohl besteht die Gefahr, dass ohne erfahrene Moderation Ursachenketten vorzeitig abgebrochen oder nur Symptome adressiert werden. In solchen Fällen empfiehlt sich eine Kombination mit weiter verzweigten Verfahren wie dem Ishikawa-Diagramm, um systemische Zusammenhänge umfassender zu beleuchten1.
In der Wirtschaftsinformatik findet die 5-Why-Methode breite Anwendung in der Fehleranalyse von IT-Systemen, im IT-Service-Management sowie in agilen Retrospektiven. Entwickler, Projektmanager und Fachanwender nutzen die Technik, um beim Auftreten von Softwarefehlern oder wiederkehrenden Störungen die tieferen Ursprünge – etwa unklare Anforderungen, lückenhafte Testprozesse oder Kommunikationsdefizite – herauszuarbeiten und gezielt Gegenmaßnahmen zu etablieren. Auf diese Weise trägt die Fünf-Warum-Analyse dazu bei, die Stabilität, Transparenz und Effizienz informationstechnischer Abläufe nachhaltig zu verbessern1.
Literaturverzeichnis
[1] Serrat, O. (2009): The Five Whys Technique. In: Knowledge Solutions, S. 307–309. Springer, New York. [2] Liker, J. K. (2004): The Toyota Way: 14 Management Principles from the World’s Greatest Manufacturer. McGraw-Hill, New York. [3] Andersen, B. & Fagerhaug, T. (2006): Root Cause Analysis: Simplified Tools and Techniques. ASQ Quality Press, Milwaukee.

Fishbone – Diagramm
Das Fishbone-Diagramm, auch als Ishikawa-Diagramm oder Ursache-Wirkungs-Diagramm bekannt, ist ein qualitatives Analyseinstrument, das im Rahmen des modernen Qualitätsmanagements breite Anwendung findet. Es wurde in den 1960er Jahren von Kaoru Ishikawa entwickelt und zählt zu den „Sieben Qualitätstechniken“, die auf präventive Fehlervermeidung und systematische Ursachenanalyse abzielen.
Die visuelle Struktur des Diagramms erinnert an ein Fischskelett: Der „Kopf“ stellt das zu analysierende Problem dar, während die „Gräten“ die Hauptursachenkategorien symbolisieren. In der Praxis hat sich die sogenannte 6M-Methode etabliert, bei der Ursachen in sechs Kategorien unterteilt werden: Mensch, Maschine, Material, Methode, Milieu und Messung. Diese systematische Kategorisierung ermöglicht eine strukturierte Analyse komplexer Problemstellungen und fördert die ganzheitliche Betrachtung von Einflussfaktoren.
Die Anwendung erfolgt typischerweise in interdisziplinären Teams, etwa im Rahmen von Qualitätszirkeln oder Root-Cause-Workshops. Dabei werden potenzielle Ursachen durch Brainstorming gesammelt und entlang der Diagrammstruktur eingeordnet. Die Methode lässt sich effektiv mit weiteren Analyseverfahren wie der „5-Why“-Technik kombinieren, um die Ursachenhierarchie zu vertiefen.
In der wissenschaftlichen Literatur wird das Fishbone-Diagramm als besonders geeignet für die visuelle Unterstützung von Problemlösungsprozessen hervorgehoben. Es findet Anwendung in Bereichen wie Fertigung, Logistik, Gesundheitswesen und zunehmend auch in der Softwareentwicklung und dem Application Management Studien zeigen, dass die Methode nicht nur zur Fehleridentifikation beiträgt, sondern auch die Teamkommunikation und das systematische Denken fördert.

Pareto Diagramm
Das Pareto-Diagramm ist ein Standardwerkzeug der Qualitäts- und Prozessanalyse.¹ Es zeigt Ursachen (z. B. Fehlertypen) als absteigend sortierte Balken und darüber eine kumulierte Prozentkurve; so werden die wenigen wichtigen Ursachen sichtbar.¹ Das Prinzip geht auf Vilfredo Pareto zurück und wurde für das Qualitätsmanagement u. a. von Juran verbreitet.² Das Pareto-Diagramm gehört zu den „Seven Basic Tools“ und ist einfach zu verstehen und anzuwenden.¹ ²
Abb. X: Pareto-Diagramm (eigene Darstellung in Anlehnung an Montgomery).¹
Der Ablauf ist klar strukturiert. Zuerst werden die Kategorien festgelegt und die Messgröße gewählt (Häufigkeit oder—besser—Wirkung wie Zeit- bzw. Kostenimpact).¹ Anschließend werden die Daten gesammelt, bereinigt und nach Beitrag sortiert; sehr kleine Restkategorien werden zu „Sonstige“ zusammengefasst.¹ Zum Schluss wird die kumulierte Kurve eingezeichnet und ein sinnvoller Cut-off für die wichtigsten Ursachen festgelegt (je nach Ziel typischerweise 70–90 %).¹ ²
Beim Interpretieren gilt: Die oft genannte „80/20-Regel“ ist eine  Faustregel, kein Gesetz; die Schwelle hängt vom Kontext ab (z. B. Kosten statt bloßer Häufigkeit).³ Für eine saubere Darstellung sollte man auf klare Achsen, korrekte Kumulierung und gut definierte Kategorien achten; dazu gibt es methodische Hinweise aus der Statistik.⁴
Bezug zur Projektarbeit (Process Mining, Logistik). Abweichungen aus dem Event-Log (z. B. Rework, SLA-Verstöße, Wartezeiten) werden Kategorien zugeordnet (z. B. Mensch, Maschine, Material, Methode, Umfeld, Messung). Ein kostenbasiertes Pareto (z. B. Minutenverzug × Volumen × Kostensatz) setzt Prioritäten dort, wo der größte Nutzen liegt; die Top-Ursachen werden anschließend mit 5-Why/Ishikawa vertieft.¹ ²
Vorteile
– Einfach und schnell umsetzbar.¹
–Liefert klare Prioritäten („wichtige wenige“).¹ ²
– Gut für große Datenmengen und Management-Kommunikation.¹
Nachteile
– 80/20 ist nur eine Heuristik; kann in die Irre führen.³
– Kleine, aber kritische Risiken können untergehen.¹
– Ergebnis hängt von Kategorienbildung und Datenqualität ab.¹
– Klassische Darstellung berücksichtigt Unsicherheit kaum.⁴
Tab. X: Vor- und Nachteile Pareto-Diagramm.
________________________________________
Quellenzuordnung (für deine Fußnoten/Literaturverzeichnis):
¹ Montgomery, D. C. (2013): Introduction to Statistical Quality Control, 7. Aufl., Wiley (Kap. 5).
² Ishikawa, K. (1982): Guide to Quality Control, Asian Productivity Organization (Kap. 5).
³ Juran, J. M.; De Feo, J. A. (2010): Juran’s Quality Handbook, 6. Aufl., McGraw-Hill (Appendix „The Non-Pareto Principle; Mea Culpa“).
⁴ Wilkinson, L. (2006): „Revising the Pareto Chart“, The American Statistician, 60(4), 332–334.

2.4 Auswirkungsbewertung 
Kosten-Nutzen-Analyse
Die Kosten-Nutzen-Analyse (KNA) ist ein klassisches Verfahren der betriebswirtschaftlichen Entscheidungstheorie und hat sich insbesondere in der Investitionsrechnung sowie in der Bewertung öffentlicher Projekte etabliert. Ziel der Methode ist es, sämtliche Kosten und Nutzen einer Maßnahme systematisch zu erfassen, zu monetarisieren und vergleichbar zu machen, um eine rationale Entscheidungsgrundlage zu schaffen^1. Dabei wird der Nutzen nicht ausschließlich in finanziellen Größen erfasst, sondern umfasst auch qualitative Effekte wie Prozessoptimierungen, Qualitätssteigerungen oder Risikoreduktionen, die durch geeignete Bewertungsverfahren in monetäre Größen transformiert werden^2.
Das methodische Vorgehen einer KNA lässt sich in mehrere Schritte gliedern: Zunächst werden die relevanten Handlungsalternativen definiert und deren Betrachtungszeitraum festgelegt. Im zweiten Schritt erfolgt die Identifikation der Kosten- und Nutzenkategorien. Hierzu zählen direkte Faktoren wie Investitions- und Betriebskosten ebenso wie indirekte Effekte, etwa Effizienzgewinne oder die Reduktion von Prozessabweichungen. Anschließend werden diese Faktoren, soweit möglich, quantifiziert und in monetäre Werte überführt. Für schwer messbare Nutzenkomponenten kommen Verfahren wie Zahlungsbereitschaftsanalysen, Schattenpreise oder Punktbewertungsverfahren zum Einsatz^3.
Darauf folgt die Diskontierung zukünftiger Kosten und Nutzen mithilfe eines geeigneten Kalkulationszinssatzes, um ihren heutigen Wert (Barwert) zu bestimmen. Durch diese Kapitalwertmethode können unterschiedliche Zeitpunkte und Nutzungsdauern vergleichbar gemacht werden. Im nächsten Schritt werden zentrale Kennzahlen berechnet, darunter der Nettonutzen (Differenz von Nutzen und Kosten), das Kosten-Nutzen-Verhältnis sowie gegebenenfalls der interne Zinsfuß. Diese Kennzahlen ermöglichen den Vergleich mehrerer Alternativen und erleichtern die Priorisierung von Projekten^4.
In der Praxis der Logistik wird die KNA insbesondere zur Bewertung von Digitalisierungs- und Automatisierungsvorhaben wie Process-Mining-Initiativen eingesetzt. Aufgrund der hohen Investitionskosten und unsicheren Nutzenpotenziale dient die KNA nicht nur der ökonomischen Bewertung, sondern auch der transparenten Kommunikation gegenüber Stakeholdern. Kritisch wird jedoch darauf hingewiesen, dass Ergebnisse stark von den getroffenen Annahmen abhängen und nicht-monetäre Faktoren wie Mitarbeiterzufriedenheit oder ökologische Effekte nur eingeschränkt abgebildet werden können. Daher empfiehlt die Literatur eine Kombination mit ergänzenden Methoden wie Nutzwert- oder Risikoanalysen, um eine ganzheitliche Entscheidungsbasis zu schaffen^5.
________________________________________
^1 Boardman, Anthony E.; Greenberg, David H.; Vining, Aidan R.; Weimer, David L. 2018. Cost-Benefit Analysis: Concepts and Practice. 5. Aufl. Cambridge: Cambridge University Press, S. 3 ff.
^2 Hanley, Nick; Barbier, Edward B. 2009. Pricing Nature: Cost-Benefit Analysis and Environmental Policy. Cheltenham: Edward Elgar, S. 17 ff.
^3 Mishan, Edward J.; Quah, Euston 2020. Cost-Benefit Analysis. 6. Aufl. New York: Routledge, S. 45 ff.
^4 Florio, Massimo 2014. Applied Welfare Economics: Cost-Benefit Analysis of Projects and Policies. London: Routledge, S. 62 ff.
^5 Günther, Holger; Krcmar, Helmut 2018. »Wirtschaftlichkeitsbewertung von Digitalisierungsvorhaben in der Logistik«, in HMD Praxis der Wirtschaftsinformatik 55, 1, S. 25–37.
FMEA
Die Fehlermöglichkeits- und Einflussanalyse (FMEA) ist eine präventive Methode des Qualitätsmanagements, die systematisch potenzielle Fehler in Produkten, Prozessen oder Systemen identifiziert und bewertet. Ursprünglich in den 1940er Jahren in der US-amerikanischen Luft- und Raumfahrt entwickelt, hat sie sich seit den 1960er Jahren – insbesondere durch die Automobilindustrie – als international anerkannte Standardmethode etabliert (Stamatis 2003, S. 15). Ziel der FMEA ist es, Risiken bereits in frühen Entwicklungs- und Planungsphasen zu erkennen, deren Ursachen zu analysieren und durch präventive Maßnahmen zu minimieren.
Das methodische Vorgehen erfolgt in mehreren Schritten. Zunächst werden die zu untersuchenden Prozesse oder Produkte strukturiert in Teilfunktionen zerlegt. Für jede Funktion werden potenzielle Fehlermöglichkeiten, deren Ursachen und Auswirkungen identifiziert. Anschließend werden diese Fehler nach drei Kriterien bewertet: Auftretenswahrscheinlichkeit, Bedeutung der Auswirkung und Entdeckungswahrscheinlichkeit. Durch Multiplikation dieser Werte entsteht die sogenannte Risiko-Prioritätszahl (RPZ), die eine Rangfolge der Risiken ermöglicht (AIAG/VDA 2019, S. 42). Hohe RPZ-Werte weisen auf besonders kritische Fehlermöglichkeiten hin, die vorrangig behandelt werden müssen.
Ein wesentlicher Vorteil der FMEA liegt in ihrer präventiven Ausrichtung: Anstatt Fehler im Nachhinein zu korrigieren, werden sie bereits im Vorfeld systematisch vermieden. Darüber hinaus fördert die Methode die interdisziplinäre Zusammenarbeit, da die Bewertung typischerweise in Teams aus verschiedenen Fachbereichen erfolgt. Gleichwohl ist kritisch anzumerken, dass die Bewertungskriterien teilweise subjektiv geprägt sind und die RPZ nicht immer eine differenzierte Risikoabwägung zulässt (Liu et al. 2013, S. 812). Neuere Ansätze schlagen daher ergänzende Methoden wie Fuzzy-Logik oder multikriterielle Entscheidungsmodelle vor, um die Genauigkeit der Risikobewertung zu erhöhen.
Im Kontext der Logistik und insbesondere von Process-Mining-Initiativen kann die FMEA genutzt werden, um die Auswirkungen identifizierter Abweichungen systematisch zu bewerten. So lassen sich nicht nur die häufigsten Ursachen (z. B. aus Pareto-Analysen), sondern auch deren potenzielle Folgen für Durchlaufzeiten, Kosten oder Servicelevel strukturieren und priorisieren. Damit stellt die FMEA ein wichtiges Bindeglied zwischen Ursachenanalyse und Maßnahmenplanung dar.
________________________________________
Quellen (für Fußnoten im Leviathan-Stil):
•	Stamatis, D. H. 2003. Failure Mode and Effect Analysis: FMEA from Theory to Execution. 2. Aufl. Milwaukee: ASQ Quality Press.
•	AIAG/VDA 2019. FMEA-Handbuch. 1. Aufl. Detroit: Automotive Industry Action Group / Verband der Automobilindustrie.
•	Liu, Hu-Chen; Liu, Li; Liu, Nan 2013. „Risk evaluation approaches in failure mode and effects analysis: A literature review“, in Expert Systems with Applications 40, 2, S. 828–838.
